{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c83dbb",
   "metadata": {},
   "source": [
    "# Calculation example for Stochastic Gradient Descent and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b5e8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can do the exercise on paper! Read through the exercise carefully. The calculations start at section 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfbcea",
   "metadata": {},
   "source": [
    "## 1) Prepare a dataset for the linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1e419",
   "metadata": {},
   "source": [
    "### 1.1) Example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51e7f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset (x values)\n",
    "X = [\n",
    "    [8, 2],\n",
    "    [2, 8],\n",
    "    [1, 4],\n",
    "    [3, 6],\n",
    "    [9, 4],\n",
    "    [6, 5]\n",
    "]\n",
    "\n",
    "# Dependent variable on X (output of the model)\n",
    "# first row in X corresponds to first row in y and so forth\n",
    "y = [\n",
    "    4,\n",
    "    -14,\n",
    "    -7,\n",
    "    -9,\n",
    "    1,\n",
    "    -4\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6824d",
   "metadata": {},
   "source": [
    "### 1.2) Depending on the dataset we think that a Linear Regression model fits the best to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f34e3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The formular for two independent input variables for linear regresion is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28e65e",
   "metadata": {},
   "source": [
    "$y_{hat}= w_1x_1 + w_2x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4527b",
   "metadata": {},
   "source": [
    "### 1.3) We need a function to evaluate to model outcome - Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "586cc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example we define our loss function to be the 'Mean Squared Error' function.\n",
    "# Keep in mind that we are not bound to this method and that we can choose 'whatever' function we want to use.\n",
    "# The function should be a representative function to estimate the models performance (How good is my model?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e49c7",
   "metadata": {},
   "source": [
    "$$L = MSE = \\frac{1}{n}\\sum_{i=1}^n (y_{i_{hat}}-y_{i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "673b1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general for classical approaches: The lower the loss the better the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4d9cc",
   "metadata": {},
   "source": [
    "### 1.4) How can we minimize the loss of our model? - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bdbf2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is were Stochastic Gradient Descent comes into play!\n",
    "# We need to calculate the gradients of our model parameters (w_1, w_2, b) and update their values!\n",
    "\n",
    "# Calculation follows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b445a",
   "metadata": {},
   "source": [
    "## 2) How to use all the information from above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791fdb2",
   "metadata": {},
   "source": [
    "### 2.1) Initialize random values for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19475f3",
   "metadata": {},
   "source": [
    "$w_1, w_2, b$\n",
    "\n",
    "e. g.\n",
    "\n",
    "$w_1 = -0.5$, $w_2 = 0.02$, $b = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20a9b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter value initialization is only done once (at the beginning)\n",
    "# We need these random values because we don't know anything about them at the beginning\n",
    "\n",
    "# Initialised random parameter values\n",
    "w_1 = -0.5\n",
    "w_2 = 0.02\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63fa7c",
   "metadata": {},
   "source": [
    "### 2.2) Use the parameter values and one data row of the dataset and calculate the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc37be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for example the first row of X as x_1 and x_2 values and\n",
    "# calculate the output with the initially set parameter values w_1, w_2, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "616e0bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First iteration: x_1=8, x_2=2, y=4\n",
      "Output with w_1=-0.5, w_2=0.02, b=0 is -3.96\n"
     ]
    }
   ],
   "source": [
    "# TODO !\n",
    "\n",
    "# First iteration\n",
    "x_1, x_2 = X[0]\n",
    "y_i = y[0]\n",
    "\n",
    "# Compute the predicted output\n",
    "y_hat = w_1 * x_1 + w_2 * x_2 + b\n",
    "\n",
    "print(f\"First iteration: x_1={x_1}, x_2={x_2}, y={y_i}\")\n",
    "print(f\"Output with w_1={w_1}, w_2={w_2}, b={b} is {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720932d",
   "metadata": {},
   "source": [
    "### 2.3) Calculate the loss of the model - How good is our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66a7ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to find out how good our model is regarding our defined loss function\n",
    "# Therefore, calculate to loss of our model based on the defined loss function above\n",
    "# Use the correct y from the data (e. g. if you use the first row X[0] then you have to use y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b203e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 63.3616\n"
     ]
    }
   ],
   "source": [
    "# TODO !\n",
    "n = 1 # first iteration\n",
    "\n",
    "loss = 1/n * (y_hat - y_i) ** 2\n",
    "\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b048e0b",
   "metadata": {},
   "source": [
    "### 2.4) Calculate the gradients of the learning parameters (w_1, w_2 and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fff298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradients can be calculated using the partial derivatives with respect to the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0d11e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with the derivation of our loss function because this is our evaluation function.\n",
    "# The loss function gives us information about how good our model performs\n",
    "# We want to update our parameters based on our quality measurement function (loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0b87fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partially derive the loss function (you can write the end result in code)\n",
    "\n",
    "# TODO !\n",
    "dL_dyhat = 2 * (y_hat - y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86f34d",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial y_{hat}} = 2(y_{hat}-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b2ac392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parially derive the model function with respect to every parameter..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865c7ea",
   "metadata": {},
   "source": [
    "$y_{hat}= w_1x_1 + w_2x_2 + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61a73753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example for w_1... where x_1 from one data row the first entry (column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83093d8f",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial y_{hat}}{\\partial w_1} = x_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8c2b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this for w_1, w_2 and b with for example the first data row X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "532f89b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO !\n",
    "dyhat_dw1 = x_1\n",
    "dyhat_dw2 = x_2\n",
    "dyhat_db = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c133fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to combine the loss function derivation and all the parameter derivations to calculate the final gradient\n",
    "# this can be done by the chain rule!\n",
    "# that is backtracing the output and the finding the impact for every single parameter\n",
    "# the chain rule is just combining the partial derivatives from the end of the model (loss fn) to the parameter we search for\n",
    "# the chain rule for the paramter w_1 is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171f5ef",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial w_{1}} = \\frac{\\partial L}{\\partial y_{hat}} * \\frac{\\partial y_{hat}}{\\partial w_1}$$\n",
    "$$\\frac{\\partial L}{\\partial w_{1}} = 2(y_{hat}-y) * x_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1128beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the gradient for all parameters w_1, w_2 and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6193211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO !\n",
    "dL_dw1 = dL_dyhat * dyhat_dw1\n",
    "dL_dw2 = dL_dyhat * dyhat_dw2\n",
    "dL_db = dL_dyhat * dyhat_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48dd68",
   "metadata": {},
   "source": [
    "### 2.5) Apply the parameter update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41dabdd",
   "metadata": {},
   "source": [
    "$$\\theta := \\theta - \\eta\\Delta_{\\theta}L(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2ad7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For w_1 this looks as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94400c",
   "metadata": {},
   "source": [
    "$$w_1 := w_1 - \\eta \\space(\\frac{\\partial L}{\\partial y_{hat}} * \\frac{\\partial y_{hat}}{\\partial w_1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ffc888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets set the learning rate eta to 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ca773",
   "metadata": {},
   "source": [
    "$$\\eta = 0.05$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25818058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the new value for all parameters w_1, w_2, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "968eeda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated parameters: w1 = 5.868, w2 = 1.612, b = 0.796\n"
     ]
    }
   ],
   "source": [
    "# TODO !\n",
    "learning_rate = 0.05\n",
    "\n",
    "# New values\n",
    "w_1 -= learning_rate * dL_dw1\n",
    "w_2 -= learning_rate * dL_dw2\n",
    "b -= learning_rate * dL_db\n",
    "\n",
    "print(f'Updated parameters: w1 = {w_1}, w2 = {w_2}, b = {b}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4411c",
   "metadata": {},
   "source": [
    "## 3) Iterate this procedure with every data row from X and y to update your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3638616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: w1 = 1.0004744293882963, w2 = -1.9990895337170753, b = -0.0075577599328742326\n",
      "Loss: 2.778292840029211e-06\n"
     ]
    }
   ],
   "source": [
    "# One iteration is using all data given once\n",
    "# Epochs can be defined to iterate over all data rows multiple times\n",
    "import numpy as np\n",
    "\n",
    "# Initialised values\n",
    "w_1 = -0.05\n",
    "w_2 = 0.02\n",
    "b = 0\n",
    "learning_rate = 0.005\n",
    "epochs = 1000 # number of iterations\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# Loss function (MSE)\n",
    "def mse(y, y_pred):\n",
    "    return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "def sgd(X, y, w_1, w_2, b, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(y)):\n",
    "            x_1, x_2 = X[i]\n",
    "            y_i = y[i]\n",
    "\n",
    "            # Predicted output\n",
    "            y_hat = w_1 * x_1 + w_2 * x_2 + b\n",
    "\n",
    "            # Compute gradients\n",
    "            # - partial derivate loss function\n",
    "            dL_dyhat = 2 * (y_hat - y_i)\n",
    "\n",
    "            # - partial derivate model\n",
    "            dyhat_dw1 = x_1\n",
    "            dyhat_dw2 = x_2\n",
    "            dyhat_db = 1\n",
    "\n",
    "            # - gradients\n",
    "            dL_dw1 = dL_dyhat * dyhat_dw1\n",
    "            dL_dw2 = dL_dyhat * dyhat_dw2\n",
    "            dL_db = dL_dyhat * dyhat_db\n",
    "\n",
    "            # Update parameters\n",
    "            w_1 -= learning_rate * dL_dw1\n",
    "            w_2 -= learning_rate * dL_dw2\n",
    "            b -= learning_rate * dL_db\n",
    "        \n",
    "        y_pred = w_1 * X[:, 0] + w_2 * X[:, 1] + b\n",
    "        loss = mse(y, y_pred)\n",
    "\n",
    "    return w_1, w_2, b, loss\n",
    "\n",
    "# Train the model\n",
    "w_1, w_2, b, loss = sgd(X, y, w_1, w_2, b, learning_rate, epochs)\n",
    "print(f'Final parameters: w1 = {w_1}, w2 = {w_2}, b = {b}')\n",
    "print(f\"Loss: {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
