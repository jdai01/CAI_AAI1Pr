{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865d7567",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f1031",
   "metadata": {},
   "source": [
    "Implement a logistic (binary) regression model and use your stochastic gradient descent approach from the last practicals to optimize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910c0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can import your code from a different notebook like follows (change it to your path)\n",
    "\n",
    "# %run ..\\5_SGD_Linear_Regression\\SGD_Solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc5873",
   "metadata": {},
   "source": [
    "## Data for the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d124ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743256fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting data for training and test are in (X_train, y_train) and (X_test, y_test), respectively.\n",
    "\n",
    "# Lets assume we have some data points that define the data as 'fraud' or 'no fraud', e. g. in bank account scenarios.\n",
    "# We therefore have three values given ('number of withdrawals per week', 'avg_withdrawal_amount', 'number_of_different_addressees').\n",
    "\n",
    "data_amount = 15\n",
    "frauds_amount = data_amount // 2\n",
    "no_frauds_amount = data_amount - frauds_amount\n",
    "\n",
    "### Generate (fake) 'fraud' data\n",
    "# number of withdrawals per week (fraud) - between 0 and 0.5\n",
    "X_1_f = np.random.rand(frauds_amount) / 2\n",
    "# average withdrawal amount 'fraud' - between 0.5 and 1\n",
    "X_2_f = (np.random.rand(frauds_amount) + 1) / 2\n",
    "# number of different addressees 'fraud' - between 0 and 0.5\n",
    "X_3_f = np.random.rand(frauds_amount) / 2\n",
    "\n",
    "X_f = np.stack([X_1_f, X_2_f, X_3_f], axis=1)\n",
    "\n",
    "# Labels of 'fraud' (1)\n",
    "y_f = np.ones(frauds_amount)\n",
    "\n",
    "### Generate (fake) 'no fraud' data - between 0.5 and 1\n",
    "X_1_nf = (np.random.rand(no_frauds_amount) + 1) / 2\n",
    "# average withdrawal amount 'no fraud' - between 0 and 0.5\n",
    "X_2_nf = np.random.rand(no_frauds_amount) / 2\n",
    "# number of different addressees 'no fraud' - between 0.5 and 1\n",
    "X_3_nf = (np.random.rand(no_frauds_amount) + 1) / 2\n",
    "\n",
    "X_nf = np.stack([X_1_nf, X_2_nf, X_3_nf], axis=1)\n",
    "\n",
    "# Labels of 'no fraud' (0)\n",
    "y_nf = np.zeros(no_frauds_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192ec997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_f = [[0.29168211 0.51926307 0.26936188]\n",
      " [0.35151149 0.98409152 0.31932494]\n",
      " [0.46512513 0.81715354 0.1295525 ]\n",
      " [0.02594845 0.65268757 0.29398976]\n",
      " [0.14478214 0.93431523 0.02151472]\n",
      " [0.32089127 0.88690067 0.0470629 ]\n",
      " [0.47828694 0.7788369  0.26245831]]\n",
      "y_f = [1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Show dataset of 'fraud' data\n",
    "print('X_f =', X_f)\n",
    "print('y_f =', y_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e7047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_nf = [[0.91175767 0.43251488 0.76972269]\n",
      " [0.97957189 0.11529604 0.9051713 ]\n",
      " [0.73743688 0.28186576 0.58604385]\n",
      " [0.60848767 0.06334995 0.8623376 ]\n",
      " [0.63037444 0.07323393 0.84732089]\n",
      " [0.68329046 0.29051951 0.6840774 ]\n",
      " [0.82340603 0.09748259 0.7003152 ]\n",
      " [0.81534687 0.20428444 0.73327069]]\n",
      "y_nf = [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Show dataset of 'no fraud' data\n",
    "print('X_nf =', X_nf)\n",
    "print('y_nf =', y_nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d8549",
   "metadata": {},
   "source": [
    "## Shuffle fraud and no fraud data to create a mixed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them (concatenate)\n",
    "X = np.concatenate((X_f, X_nf))\n",
    "y = np.concatenate((y_f, y_nf))\n",
    "\n",
    "# now randomly shuffle them\n",
    "shuffled_indices = np.random.choice(y.shape[0], size=y.shape[0], replace=False)\n",
    "X = X[shuffled_indices]\n",
    "y = y[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d5ca7",
   "metadata": {},
   "source": [
    "## Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4d5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(data_amount * 0.75)\n",
    "\n",
    "# We train with the following data\n",
    "X_train = X[:train_len]\n",
    "y_train = y[:train_len]\n",
    "\n",
    "# We test / evaluate with the following data\n",
    "X_test = X[train_len:]\n",
    "y_test = y[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2880fc3",
   "metadata": {},
   "source": [
    "## Initialize the weights of your logistic regression model (see SGD exercise on how to do this with numpy - dont forget to initialize the bias nodes as well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155071b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights\n",
    "w = np.random.randn(X_train.shape[1])\n",
    "\n",
    "# bias\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff9268",
   "metadata": {},
   "source": [
    "## Define the loss function (derivative) for your logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb5990",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy Loss function:\n",
    "$$L = -\\frac{1}{N} \\sum^{N}_{i=1} (y_i \\cdot \\log(y_{pred, i})) - (1-y_i) \\cdot \\log(1-y_{pred, i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe131ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Binary Cross Entropy\n",
    "\n",
    "def binary_cross_entropy(y, y_pred):\n",
    "    epsilon = 1e-15 # to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) # clipping y_pred where it will never be 0 [epsilon, 1-epsilon]\n",
    "    \n",
    "    return -np.mean( y * np.log(y_pred) - (1-y) * np.log(1-y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e102bd",
   "metadata": {},
   "source": [
    "## Define the activation function for your logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f747d",
   "metadata": {},
   "source": [
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a13e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint:  How do you scale your output for logistic regression? What is the range?\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Range: [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed072e4",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} & = \\frac{y_i}{\\hat{y}} - \\frac{1 - y_i}{1 - \\hat{y}}\\\\\n",
    "    & = \\frac{y_i (1 - \\hat{y}) - \\hat{y} (1 - y_i)}{\\hat{y} (1 - \\hat{y})}\\\\\n",
    "    & = \\frac{y_i - y_i \\hat{y} - \\hat{y} + y_i \\hat{y}}{\\hat{y} (1 - \\hat{y})}\\\\\n",
    "    & = \\frac{y_i - \\hat{y}}{\\hat{y} (1 - \\hat{y})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20631016",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{\\partial \\hat{y}}{\\partial z} & = \\frac{\\partial}{\\partial z} (1 + e^{-z})^{-1}\\\\\n",
    "    & = (1 + e^{-z})^{-2} \\cdot -1 \\cdot e^{-z} \\cdot -1\\\\\n",
    "    & = \\frac{e^{-z}}{(1 + e^{-z})^2}\\\\\n",
    "    & = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}}\\\\\n",
    "    & = \\hat{y} \\cdot (1 - \\hat{y})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f5f39",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial z} & = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}\\\\\n",
    "    & = \\frac{y_i - \\hat{y}}{\\hat{y} (1 - \\hat{y})} \\cdot (\\hat{y} \\cdot (1 - \\hat{y}))\\\\\n",
    "    & = (y_i - \\hat{y})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960589e9",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    z & = w \\cdot x + b\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial z}{\\partial w} & = x\\\\\n",
    "    \\frac{\\partial z}{\\partial b} & = 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a954e0",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial w} & = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\\\\\n",
    "    & = (y_i - \\hat{y}) \\cdot x\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial b} & = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}\\\\\n",
    "    & = (y_i - \\hat{y}) \\cdot 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c133b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use your Stochastic Gradient Descent approach from the previous exercise and optimize your weights.\n",
    "# If your SGD implementation cannot do this, adjust the function implementation until it is able to do it :)\n",
    "\n",
    "learning_rate = 0.005\n",
    "iterations = 1000\n",
    "\n",
    "def sgd_logistic(X, y, w, b, learning_rate, iterations):\n",
    "    for num in range(iterations):\n",
    "        for i in range(len(y)):\n",
    "            x_i = X[i]\n",
    "            y_i = y[i]\n",
    "\n",
    "            # Predicted output\n",
    "            y_hat = sigmoid(np.dot(w, x_i) + b)\n",
    "\n",
    "            # Compute gradients\n",
    "            dL_dw = (y_i - y_hat) * x_i\n",
    "            dL_db = (y_i - y_hat)\n",
    "\n",
    "            # Update parameters\n",
    "            w -= learning_rate * dL_dw\n",
    "            b -= learning_rate * dL_db\n",
    "\n",
    "        # Calculate loss\n",
    "        y_pred = sigmoid(np.dot(X, w) + b)\n",
    "        loss = binary_cross_entropy(y, y_pred)\n",
    "\n",
    "    return w, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8927dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: w = [23.76052656  8.29844811 22.31744632], b = 29.582651400592844\n",
      "Final training loss: -18.839768723095023\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "w, b, loss = sgd_logistic(X_train, y_train, w, b, learning_rate, iterations)\n",
    "print(f'Final parameters: w = {w}, b = {b}')\n",
    "print(f\"Final training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f144a8",
   "metadata": {},
   "source": [
    "## Predict the values for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb00461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: [1. 1. 0. 0.]\n",
      "y_test_pred: [1. 1. 1. 1.]\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_test_pred = sigmoid(np.dot(X_test, w) + b)\n",
    "y_test_pred_class = (y_test_pred >= 0.5).astype(int)\n",
    "accuracy = np.mean(y_test_pred_class == y_test)\n",
    "\n",
    "print(f\"y_test: {y_test}\")\n",
    "print(f\"y_test_pred: {y_test_pred}\")\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
